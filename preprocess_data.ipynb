{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "from itertools import product\n",
    "import gc\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{0:.4f}'.format\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "ran_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''  \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:\\\\Users\\\\Desktop\\\\Sales\\\\sales_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b56a3d4af0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mshops_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPWD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shops.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:\\\\Users\\\\Desktop\\\\Sales\\\\sales_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "PWD = r\"C:\\Users\\Desktop\\Sales\"\n",
    "\n",
    "train_file = os.path.join(PWD, \"sales_train.csv\")\n",
    "test_file = os.path.join(PWD, \"test.csv\")\n",
    "\n",
    "items_file = os.path.join(PWD, \"items.csv\")\n",
    "item_categories_file = os.path.join(PWD, \"item_categories.csv\")\n",
    "shops_file = os.path.join(PWD, \"shops.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "items_df = pd.read_csv(items_file)\n",
    "item_categories_df = pd.read_csv(item_categories_file)\n",
    "shops_df = pd.read_csv(shops_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['date_block_num'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shop_item_grid(data_df, index_cols):\n",
    "\n",
    "    # For every month we create a grid from all shops/items combinations from that month\n",
    "    grid = [] \n",
    "  \n",
    "    for block_num in data_df['date_block_num'].unique():\n",
    "        cur_shops = data_df.loc[data_df['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "        cur_items = data_df.loc[data_df['date_block_num'] == block_num, 'item_id'].unique()\n",
    "        row_array = np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32')\n",
    "       \n",
    "        grid.append(row_array)\n",
    "    \n",
    "    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_monthly_sales(sales, feature_grid, index_cols):\n",
    "        \n",
    "    shop_item_mth_gb = sales.groupby(index_cols, as_index=False).agg(\n",
    "        {\"item_cnt_day\" : \"sum\"}).rename(\n",
    "        columns={\"item_cnt_day\" : \"target\"})\n",
    "        \n",
    "    shop_mth_gb = sales.groupby([\"shop_id\", \"date_block_num\"], as_index=False).agg(\n",
    "        {\"item_cnt_day\" : \"sum\"}).rename(\n",
    "        columns={\"item_cnt_day\" : \"target_shop\"})\n",
    "    \n",
    "    shop_item_gb = sales.groupby([\"item_id\", \"date_block_num\"], as_index=False).agg(\n",
    "        {\"item_cnt_day\" : \"sum\"}).rename(\n",
    "        columns={\"item_cnt_day\" : \"target_item\"})\n",
    "    \n",
    "    all_data = pd.merge(feature_grid, shop_item_mth_gb, how='left', on=index_cols).fillna(0)\n",
    "    all_data = pd.merge(all_data, shop_mth_gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "    all_data = pd.merge(all_data, shop_item_gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "    \n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(time_data, lags, cols, index_cols):\n",
    "    \n",
    "    for month_shift in lags:\n",
    "        shift_X = time_data[index_cols + cols].copy()\n",
    "        shift_X['date_block_num'] = shift_X['date_block_num'] + month_shift\n",
    "        suffix = \"_lag-{:02d}\".format(month_shift)\n",
    "        for c in cols:\n",
    "            shift_X.rename({c:\"{}{}\".format(c, suffix)}, \n",
    "                           axis=1, inplace=True)\n",
    "        time_data = pd.merge(time_data, shift_X, how='left', on=index_cols).fillna(0)\n",
    "        \n",
    "    del shift_X\n",
    "    \n",
    "    return time_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_test_to_train(train_df, test_df):\n",
    "    test_to_grid = test_df.copy().drop(['ID'], axis=1)\n",
    "    \n",
    "    train_cols = train_df.columns\n",
    "    test_cols = test_to_grid.columns\n",
    "    \n",
    "    cols_to_add = [x for x in train_cols if x not in test_cols]\n",
    "        \n",
    "    df_to_add = pd.DataFrame(np.zeros((test_df.shape[0], len(cols_to_add)), np.int32),\n",
    "                             columns=cols_to_add)\n",
    "    \n",
    "    test_to_grid = pd.concat([test_to_grid, df_to_add], axis=1, )\n",
    "       \n",
    "    test_to_grid['date_block_num'] = 34\n",
    "    all_data = train_df.append(test_to_grid, sort=False)\n",
    "    \n",
    "    return all_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_test_to_monthly_sales(monthly_sales_df, test_df):\n",
    "    \n",
    "    test_to_append = test_df.copy().drop(['ID'], axis=1)\n",
    "    \n",
    "    mth_cols = monthly_sales_df.columns\n",
    "    test_cols = test_to_append.columns\n",
    "    \n",
    "    cols_to_add = [x for x in mth_cols if x not in test_cols]\n",
    "        \n",
    "    df_to_add = pd.DataFrame(np.zeros((test_df.shape[0], len(cols_to_add)), np.int32),\n",
    "                             columns=cols_to_add)\n",
    "    \n",
    "    test_to_append = pd.concat([test_to_append, df_to_add], axis=1)\n",
    "       \n",
    "    test_to_append['date_block_num'] = 34\n",
    "    df = monthly_sales_df.append(test_to_append, sort=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_feature_matrix(train_df, test_df, items_df, lags, index_cols):\n",
    "\n",
    "    grid = create_shop_item_grid(train_df, index_cols)\n",
    "\n",
    "    all_data = convert_monthly_sales(train_df, grid, index_cols)\n",
    "    all_data = append_test_to_monthly_sales(all_data, test_df)\n",
    "    \n",
    "    all_data = add_lag_features(all_data, lags, ['target', 'target_shop', 'target_item'], index_cols)\n",
    "    \n",
    "    # Category for each item\n",
    "    item_category_mapping = items_df[['item_id','item_category_id']].drop_duplicates()\n",
    "    \n",
    "    all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "    \n",
    "    del grid\n",
    "    gc.collect()\n",
    "       \n",
    "    return all_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.item_price < 100000]\n",
    "train_df = train_df[train_df.item_cnt_day < 1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[\"item_price\"] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\n",
    "    (train_df.shop_id==32)&\n",
    "    (train_df.item_id==2973)&\n",
    "    (train_df.date_block_num==4)&\n",
    "    (train_df.item_price>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = train_df[\n",
    "    (train_df.shop_id==32)&\n",
    "    (train_df.item_id==2973)&\n",
    "    (train_df.date_block_num==4)&\n",
    "    (train_df.item_price>0)].item_price.median()\n",
    "print(\"median\", median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df[\"item_price\"] <= 0, \"item_price\"] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "train_df.loc[train_df.shop_id == 0, 'shop_id'] = 57\n",
    "test_df.loc[test_df.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train_df.loc[train_df.shop_id == 1, 'shop_id'] = 58\n",
    "test_df.loc[test_df.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train_df.loc[train_df.shop_id == 10, 'shop_id'] = 11\n",
    "test_df.loc[test_df.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gen_feature_matrix(train_df, test_df, items_df, [1, 2, 3, 6, 12], index_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price Trend Lag. Track the change of the latest price change of an item \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = train_df.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "all_data_price = pd.merge(all_data, group, on=['item_id'], how='left')\n",
    "all_data_price['item_avg_item_price'] = all_data_price['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "all_data_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = train_df.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "all_data_price = pd.merge(all_data_price, group, on=['date_block_num','item_id'], how='left')\n",
    "all_data_price['date_item_avg_item_price'] = all_data_price['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "all_data_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1,2,3,4,5,6]\n",
    "all_data_price = add_lag_features(all_data_price, lags, ['date_item_avg_item_price'], index_cols)\n",
    "all_data_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the price change of an item compare with the mean price along lagged months. Normalize the price change\n",
    "for i in lags:\n",
    "    all_data_price['delta_price_lag_'+str(i)] =  (all_data_price['date_item_avg_item_price_lag-'+ \"{:02d}\".format(i) ] - all_data_price['item_avg_item_price']) / all_data_price['item_avg_item_price']\n",
    "    \n",
    "all_data_price[all_data_price.date_block_num == 12].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the nearest month with price change\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if not row['delta_price_lag_'+str(i)] == 0:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data_price['delta_price_lag'] = all_data_price.apply(select_trend, axis=1)\n",
    "all_data_price['delta_price_lag'] = all_data_price['delta_price_lag'].astype(np.float16)\n",
    "all_data_price['delta_price_lag'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "fetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    fetures_to_drop += ['date_item_avg_item_price_lag-{:02d}'.format(i)]\n",
    "    fetures_to_drop += ['delta_price_lag_'+str(i)]\n",
    "                        \n",
    "all_data_price.drop(fetures_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data_price\n",
    "all_data_price = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = all_data.groupby(['date_block_num']).agg({'target': ['mean']})\n",
    "grouped.columns = [ 'date_avg_target' ]\n",
    "grouped.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "all_data = pd.merge(all_data, grouped, on=['date_block_num'], how='left')\n",
    "all_data['date_avg_target'] = all_data['date_avg_target'].astype(np.float16)\n",
    "all_data = add_lag_features(all_data, [1], ['date_avg_target'], index_cols)\n",
    "all_data.drop(['date_avg_target'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped = all_data.groupby(['date_block_num', 'item_id']).agg({'target': ['mean']})\n",
    "grouped.columns = [ 'date_item_avg_target' ]\n",
    "grouped.reset_index(inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, grouped, on=['date_block_num','item_id'], how='left')\n",
    "all_data['date_item_avg_target'] = all_data['date_item_avg_target'].astype(np.float16)\n",
    "all_data = add_lag_features(all_data, [1,2,3,6,12], ['date_item_avg_target'], index_cols)\n",
    "all_data.drop(['date_item_avg_target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = all_data.groupby(['date_block_num', 'shop_id']).agg({'target': ['mean']})\n",
    "grouped.columns = [ 'date_shop_avg_target' ]\n",
    "grouped.reset_index(inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, grouped, on=['date_block_num','shop_id'], how='left')\n",
    "all_data['date_shop_avg_target'] = all_data['date_shop_avg_target'].astype(np.float16)\n",
    "all_data = add_lag_features(all_data, [1,2,3,6,12], ['date_shop_avg_target'], index_cols)\n",
    "all_data.drop(['date_shop_avg_target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = all_data.groupby(['date_block_num', 'item_category_id']).agg({'target': ['mean']})\n",
    "grouped.columns = [ 'date_item_category_avg_target' ]\n",
    "grouped.reset_index(inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, grouped, on=['date_block_num','item_category_id'], how='left')\n",
    "all_data['date_item_category_avg_target'] = all_data['date_item_category_avg_target'].astype(np.float16)\n",
    "all_data = add_lag_features(all_data, [1], ['date_item_category_avg_target'], index_cols)\n",
    "all_data.drop(['date_item_category_avg_target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = all_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'target': ['mean']})\n",
    "grouped.columns = [ 'date_shop_item_category_avg_target' ]\n",
    "grouped.reset_index(inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, grouped, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\n",
    "all_data['date_shop_item_category_avg_target'] = all_data['date_shop_item_category_avg_target'].astype(np.float16)\n",
    "all_data = add_lag_features(all_data, [1], ['date_shop_item_category_avg_target'], index_cols)\n",
    "all_data.drop(['date_shop_item_category_avg_target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"date_block_num\"] == 12].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of months since of the first sale of a shop-item pair and an item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_month_shop_item = all_data.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "first_month_item = all_data.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n",
    "\n",
    "all_data['item_shop_first_sale'] = all_data['date_block_num'] - first_month_shop_item\n",
    "all_data['item_first_sale'] = all_data['date_block_num'] - first_month_item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The month and number of days in that month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['month'] = all_data['date_block_num'] % 12\n",
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "all_data['days'] = all_data['month'].map(days).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the first year samples as they have no lagged history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[all_data[\"date_block_num\"] >= 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the target_shop and target_item as they are the info of current month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop([\"target_shop\", \"target_item\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all_data shape:\", all_data.shape)\n",
    "all_data.to_csv(\"all_data_lag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write the shop and item ids pair sequence for level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_item_pairs = all_data[[\"shop_id\", \"item_id\", \"date_block_num\"]] \n",
    "shop_item_pairs.to_csv(\"all_data_shop_item_pairs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
